library(dataset)
library(datasets)
data(iris)
?iris
sapply(iris,Sepal.Length,mean)
sapply(iris,"Sepal.Length",mean)
sapply(iris,1,mean)
apply(iris,1,mean)
apply(iris, Sepal.Length, mean)
tapply(iris, 1,mean)
lapply(iris,Sepal.Length, mean)
tapply(iris, 1, mean)
lapply(iris, 1, mean)
lapply(iris,mean
)
iris
colmeans(iris)
colMeans(iris)
apply(iris[, 1:4], 1, mean)
apply(iris[, 1:4], 2, mean)
library(datasets)
data(mtcars)
tapply(mtcars$cyl, mtcars$mpg, mean)
lapply(mtcars, mean)
with(mtcars, tapply(mpg, cyl, mean))
with(mtcars, tapply(hp, cyl, mean))
with(iris, lapply(Species, Sepal.Length, mean)
)
tapply(iris,mean)
lapply(iris,mean)
newdata <- iris[ which(Species == "virginica"),]
newdata <- iris[ ,which(Species == "virginica")]
iris
iris$Species
newdata <- subset(iris, Species=="virginica)
newdata <- subset(iris, Species == "virginica")
lapply(newdata,mean)
sapply(split(mtcars$mpg, mtcars$cyl), mean)
apply(mtcars, 2, mean)
mean(mtcars$mpg, mtcars$cyl)
tapply(mtcars$cyl, mtcars$mpg, mean)
set.seed(1)
rpois(5,2)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv",destfile="testfile.csv",method="curl")
list.files
list.files()
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv",destfile="/testfile.csv",method="curl")
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
r.version.string
R.version.string
library(swirl)
swirl()
install_from_swirl("Getting and Cleaning Data")
swirl()
read.csv(path2csv,stringAsFactors=FALSE)
read.csv(path2csv,stringsAsFactors=FALSE)
mydf <- read.csv(path2csv,stringsAsFactors=FALSE)
dim()
mydf.dim()
dim(mydf)
head(mydf)
library(dplyr)
packageVersion("dplyr")
cran <- tbl_df(mydf)
rm("mydf")
?tbl_df
cran
?select
select(cran,ip_id,package, country)
5:20
select(cran, r_arch:country)
select(cran, country:r_arch)
cran
select(cran,-time)
select(cran, -5:20)
-5:20
-(5:20)
select(cran,-(5:20))
select(cran,-(x:size))
select(cran, -(x:size))
select(cran, -(X:size))
filter(cran, package=="swirl")
filter(cran, r_version=="3.1.1",country=="us")
filter(cran, r_version=="3.1.1",country=="US")
?comparison
?Comparison
filter(cran, r_version=="3.1.1",country IN ("India"))
filter(cran, r_version<="3.0.2",country == "IN")
fitler(Cran, country=="US"|country=="IN")
filter(Cran, country=="US"|country=="IN")
filter(cran, country=="US"|country=="IN")
filter(cran, size>100500, r_os == "linux-gnu")
is.na(c(3,5,NA,10))
!is.na(c(3,5,NA,10))
filter(cran, !is.na(r_version))
cran2 <- select(cran, size:ip_id)
arrange(cran2,ip_id)
arrange(cran2,desc(ip_id)
)
arrange(cran2,package,ip_id)
arrange(cran2, country, desc(r_version), ip_ID)
arrange(cran2, country, desc(r_version), ip_id)
cran3 <- select(cran,ip_id,package,size)
cran3
mutate(cran3, size_mb=size/2^20)
mutate(cran3,size_gb=size_mb/2^10)
mutate(cran3, size_mb=size/2^20, size_gb=size_mb/2^10)
mutate(cran3, correct_size=size+1000)
summarize(cran,avg_bytes=mean(size))
library(dplyr)
cran <- tbl_df(mydf)
rm("mydf")
cran
?group_by()
?group_by
by_package <- group_by(cran, package)
by_package
summarize(by_package,mean(size))
submit()
pack_sum
quantile(pack_sum$count, probs=0.99)
top_counts <- filter(pack_sum, count>69.56)
top_counts <- filter(pack_sum, couunt > 679)
top_counts <- filter(pack_sum, count > 679)
top_counts
view(top_counts)
View(top_counts)
top_counts_sorted <- arrange(top_count, desc(count)
)
top_counts_sorted <- arrange(top_counts, desc(count)
)
view()
View(top_counts_sorted))
View(top_counts_sorted)
quantile(pack_sum$unique, probs=0.99)
top_unique <- filter(pack_sum, unique>465)
View(top_Unique)
View(top_unique)
top_unique_sorted <- arrange(top_unique, desc(unique))
View(top_unique_sorted)
submit()
submit9)
submit()
submit()
View(result3)
submit()
submit()
submit()
submit()
submit()
submit()
?mutate
submit()
?mutate
submit()
submit()
library(tidyr)
students
?gather
gather(students, sex,count,-grade)
students2
gather(students2,sex_class, count, -grade)
res <-0 gather(students2, sex_class, count, -grade)
res <- gather(students2, sex_class, count, -grade)
res
?separate
separte(data=res, col=sex_class, into=c("sex","class"))
separate(data=res, col=sex_class, into=c("sex","class"))
submit()
students3
?gather
submit()
submit()
?spread
submit()
submit()
extract_numeric("class5")
extract_numeric("class5")
submit()
students4
submit()
submit()
submit()
submit()
passed
failed
passed <- mutate(passsed, status=c("A","B"))
passed <- mutate(passed, status=c("A","B"))
passed <- mutate(passed, final=c("A","B"))
passed <- mutate(passed, final="A"|final="B")
passed <- mutate(passed, final=="A"|final=="B")
passed <- mutate(passed, "passed")
passed <- passed %>% mutate(status = "passed")
failed <- failed %>% mutate(status = "failed")
bind_rows(passed,failed)
sat
?separate
submit()
submit()
swirl(0)
swirl()
library(swirl)
swirl(0)
Sys.getlocale("LC_TIME")
library(lubridate)
help(package=lubridate)
today(0)
today()
this_day <- today(0)
this_day <- today()
this_day
year(this_day)
wday(this_day)
wday(this_day,label=TRUE)
this_moment <- now()
this_moment
second(this_moment)
ymd("1989-05-17")
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
mdy("March 12, 1975")
dmy(25081985)
ymd("192012")
ymd("--192012")
ymd("1920/1/2")
dt1
ymd_hms(dt1)
hms("03:22:14")
dt2
ymd(dt2)
update(this_moment,hours=8,minutes=34,seconds=55)
this_moment
this_moment <- update(this_moment, now())
this_moment <- update(this_moment, hours=10, minutes=16, seconds=0)
this_moment
nyc <- now("America/New_York")
nyc
depart <- nyc + days(2)
depart
update(depart, hours=17, minutes=34)
update(depart, h=17, m=34)
depart <- update(depart, hours=17, minutes=34)
depart
arrive <- depart + hours(15) + minutes(50)
?with_tz
arrive <- with_tz(arrive, "Asia/Hong-Kong")
arrive <- with_tz(arrive, "Asia/Hong_Kong")
arrive
last_time <- with_tz(mdy("June 17, 2008"), "Singapore")
last_time <- mdy("June 17, 2008", "Singapore")
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time
?new_interval()
?new_interval
how_long <- new_interval(last_time, arrive)
as.period(how_long)
stopwatch(0)
stopwatch()
install_from_swirl("Statistical Inference")
swirl(0)
33/36
deck
52
4/52
0
16/52
12/52
2/51
library(swirl)
swirl(0)
.75*.8
.8*.8
.64
mypdf
integrate(mypdf,lower=0,upper=1.6)
quantile(mypdf,probs=50)
quantile(mypdf,probs=c(50))
14.14
.14
sqrt(2)
.997*.001
.015*.999
.000997/(.000997+.014985)
library(swirl)
swirl()
3.5
expect_dice
dice_high
expect_dice(dice_high)
exect_dice(dice_low)
expect_dice(dice_low)
.5(edh+edl)
.5 * (edh+edl)
integrate(myfunc,0,2)
spop
mean(spop)
allsam
apply(allsam,1,mean)
mean(smeans)
dice_sqr
ex2_fair <- sum(dice_sqr*PDF)
ex2_fair <- sum(dice_sqr*dice_fair)
ex2_fair
ex2_fair - 3.5^2
ex2_fair - edh^2
sum(dice_sqr*dice_high) - edh^2
sd(apply(matrix(rnorm(10000),1000),1,mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(runif(10000),1000),1,mean))
quit()
library(swirl)
swirl()
ls()
2/sqrt(10)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/(2*sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
chose(2,5)*.8^3*.2^2
choose(2,5)*.8^3*.2^2
choose(5,2)*.8^3*.2^2
(choose(5,3)*.8^3*.2^(5-3)) + (choose(5,4)*.8^4*.2^(5-4)) + (choose(5,5)*.8^5*.2^(5-5))
pbinom(2,5,.8,lower.tail=FALSE)
qnorm(10)
qnorm(.1)
0
qnorm(.95)
qnorm(.95,mean=1100,sd=75)
(choose(5,4)*.5^4*.5^(5-4)) + (choose(5,5)*.5^5*.5^(5-5))
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
?download.file()
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv","tempfile.csv")
tempfile <- read.csv("tempfile.csv")
head(tempfile)
tempfile$ACR
tempfile$ACR = 3
head(tempfile)
tempfile$ACR == 3
tempfile <- read.csv("tempfile.csv")
tempfile$ACR == 3
agricultureLogical <- tempfile$ACR == 3 & tempfile$AGS == 6
which(agricultureLogical)[1:3]
library(jpeg)
?jpeg
img <- readJPEG("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg",native=TRUE)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg","imgfile.jpg")
img <- readJPEG("imgfile.jpg" native=TRUE)
img <- readJPEG("imgfile.jpg" native=TRUE)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
f <- file.path(getwd(), "jeff.jpg")
download.file(url, f, mode = "wb")
img <- readJPEG(f, native = TRUE)
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
f <- file.path(getwd(), "jeff.jpg")
download.file(url, f, mode = "wb")
img <- readJPEG(f, native = TRUE)
?quantile
quantile(img,c(0.3,0.8)
)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv","tmpfile2.csv")
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
f <- file.path(getwd(), "GDP.csv")
download.file(url, f)
dtGDP <- data.table(read.csv(f, skip = 4, nrows = 215))
dtGDP <- dtGDP[X != ""]
dtGDP <- dtGDP[, list(X, X.1, X.3, X.4)]
setnames(dtGDP, c("X", "X.1", "X.3", "X.4"), c("CountryCode", "rankingGDP",
"Long.Name", "gdp"))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
f <- file.path(getwd(), "EDSTATS_Country.csv")
download.file(url, f)
dtEd <- data.table(read.csv(f))
dt <- merge(dtGDP, dtEd, all = TRUE, by = c("CountryCode"))
sum(!is.na(unique(dt$rankingGDP)))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
f <- file.path(getwd(), "GDP.csv")
download.file(url, f)
dtGDP <- data.table(read.csv(f, skip = 4, nrows = 215))
dtGDP <- dtGDP[X != ""]
dtGDP <- dtGDP[, list(X, X.1, X.3, X.4)]
setnames(dtGDP, c("X", "X.1", "X.3", "X.4"), c("CountryCode", "rankingGDP",
"Long.Name", "gdp"))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
f <- file.path(getwd(), "EDSTATS_Country.csv")
download.file(url, f)
dtEd <- data.table(read.csv(f))
dt <- merge(dtGDP, dtEd, all = TRUE, by = c("CountryCode"))
sum(!is.na(unique(dt$rankingGDP)))
set.seed(3)
lambda <- 0.2
num_sim <- 1000
sample_size <- 40
sim <- matrix(rexp(num_sim*sample_size, rate=lambda), num_sim, sample_size)
row_means <- rowMeans(sim)
row_means
# plot the histogram of averages
hist(row_means, breaks=50, prob=TRUE,
main="Distribution of averages of samples,
drawn from exponential distribution with lambda=0.2",
xlab="")
# density of the averages of samples
lines(density(row_means))
# theoretical center of distribution
abline(v=1/lambda, col="red")
# theoretical density of the averages of samples
xfit <- seq(min(row_means), max(row_means), length=100)
yfit <- dnorm(xfit, mean=1/lambda, sd=(1/lambda/sqrt(sample_size)))
lines(xfit, yfit, pch=22, col="red", lty=2)
# add legend
legend('topright', c("simulation", "theoretical"), lty=c(1,2), col=c("black", "red"))
setwd("C:/Users/jhenry/Desktop/Coursera/Class - Getting and Cleaning Data")
source(run_analysis.R)
source("run_analysis.R")
